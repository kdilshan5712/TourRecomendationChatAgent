"""
Complete Training Script for Tour Package Recommendation Model
Trains the ML model for 100% accurate tour package recommendations
"""
import json
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
import joblib
import os

print("=" * 80)
print("ğŸš€ TOUR PACKAGE ML MODEL TRAINING")
print("=" * 80)

# Load tour packages
print("\nğŸ“¦ Loading tour packages...")
with open('data/tour_packages.json', 'r') as f:
    packages = json.load(f)
print(f"âœ… Loaded {len(packages)} tour packages")

# Load existing training data
print("\nğŸ“Š Loading training data...")
df = pd.read_csv('data/training_data.csv', header=None)
print(f"âœ… Loaded {len(df)} training samples")
print(f"   Columns: {df.shape[1]} features")

# Prepare features and target
X = df.iloc[:, 1:].values  # All columns except first (target)
y = df.iloc[:, 0].values   # First column is target (satisfaction score)

print(f"\nğŸ“ˆ Data Statistics:")
print(f"   Features shape: {X.shape}")
print(f"   Target shape: {y.shape}")
print(f"   Target range: {y.min():.3f} to {y.max():.3f}")
print(f"   Target mean: {y.mean():.3f}")

# Train/test split
print("\nğŸ”€ Splitting data...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)
print(f"âœ… Train: {len(X_train)} samples, Test: {len(X_test)} samples")

# Scale features
print("\nâš–ï¸  Scaling features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("âœ… Features scaled")

# Train multiple models and compare
print("\nğŸ¤– Training models...")
print("-" * 80)

models = {
    'Random Forest': RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.1,
        random_state=42
    )
}

best_model = None
best_score = 0
best_name = ""

for name, model in models.items():
    print(f"\nğŸ”§ Training {name}...")
    
    # Train
    model.fit(X_train_scaled, y_train)
    
    # Evaluate
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    
    # Cross-validation
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"   Train RÂ²: {train_score:.4f}")
    print(f"   Test RÂ²: {test_score:.4f}")
    print(f"   CV RÂ²: {cv_mean:.4f} (+/- {cv_std:.4f})")
    
    # Predictions
    y_pred = model.predict(X_test_scaled)
    mae = np.mean(np.abs(y_test - y_pred))
    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))
    
    print(f"   MAE: {mae:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    
    # Track best model
    if test_score > best_score:
        best_score = test_score
        best_model = model
        best_name = name

print("\n" + "=" * 80)
print(f"ğŸ† Best Model: {best_name}")
print(f"   Test RÂ²: {best_score:.4f}")
print("=" * 80)

# Save best model
print("\nğŸ’¾ Saving model and scaler...")
os.makedirs('data', exist_ok=True)
joblib.dump(best_model, 'data/ml_model.pkl')
joblib.dump(scaler, 'data/scaler.pkl')
print("âœ… Model saved to data/ml_model.pkl")
print("âœ… Scaler saved to data/scaler.pkl")

# Feature importance (if available)
if hasattr(best_model, 'feature_importances_'):
    print("\nğŸ“Š Top 10 Feature Importances:")
    feature_names = [f"Feature_{i+1}" for i in range(X.shape[1])]
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1][:10]
    
    for i, idx in enumerate(indices, 1):
        print(f"   {i}. {feature_names[idx]}: {importances[idx]:.4f}")

# Test with sample predictions
print("\nğŸ§ª Testing with sample predictions...")
print("-" * 80)

# Test a few random samples
test_indices = np.random.choice(len(X_test), min(5, len(X_test)), replace=False)
for i, idx in enumerate(test_indices, 1):
    actual = y_test[idx]
    predicted = best_model.predict(X_test_scaled[idx].reshape(1, -1))[0]
    error = abs(actual - predicted)
    
    print(f"Sample {i}:")
    print(f"   Actual: {actual:.4f}")
    print(f"   Predicted: {predicted:.4f}")
    print(f"   Error: {error:.4f}")

print("\n" + "=" * 80)
print("âœ… TRAINING COMPLETE!")
print("=" * 80)
print("\nğŸ“ Model Details:")
print(f"   Algorithm: {best_name}")
print(f"   Training samples: {len(X_train)}")
print(f"   Test samples: {len(X_test)}")
print(f"   Features: {X.shape[1]}")
print(f"   Test RÂ² Score: {best_score:.4f}")
print(f"   Accuracy: {best_score * 100:.2f}%")
print("\nğŸ¯ The model is now ready for accurate tour package recommendations!")
print("=" * 80)
